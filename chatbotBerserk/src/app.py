import streamlit as st
from llm_manager import LLMManager
import requests

# ConfiguraÃ§Ã£o inicial do Streamlit
st.set_page_config(
    page_title="Berserk Requirements Elicitor",
    initial_sidebar_state="expanded",
    page_icon="ğŸ¤–",
    layout="wide"
)

# Estilo CSS customizado
st.markdown("""
    <style>
        .block-container {
            padding-top: 1rem;
            padding-bottom: 1rem;
        }
        .stChatMessage {
            padding: 0.6rem 1rem;
            margin-bottom: 0.5rem;
            border-radius: 0.5rem;
        }
        .stChatMessage.user {
            background-color: #e6f7ff;
            border: 1px solid #91d5ff;
        }
        .stChatMessage.assistant {
            background-color: #f6ffed;
            border: 1px solid #b7eb8f;
        }
        .stExpander {
            border: 1px solid #ccc;
            border-radius: 0.5rem;
        }
    </style>
""", unsafe_allow_html=True)

# Verifica se o Ollama estÃ¡ rodando e se o modelo llama3.2 estÃ¡ disponÃ­vel
try:
    response = requests.get('http://localhost:11434/api/tags').json()
    if not any(model['name'].startswith('llama3.2') for model in response['models']):
        st.error("âš ï¸ Modelo llama3.2 nÃ£o encontrado. Execute 'ollama pull llama3.2' primeiro.")
        st.stop()
except Exception:
    st.error("âš ï¸ NÃ£o foi possÃ­vel conectar ao Ollama. Certifique-se que o servidor Ollama estÃ¡ rodando.")
    st.stop()

# InicializaÃ§Ã£o de estado
if 'messages' not in st.session_state:
    st.session_state.messages = [{
        "role": "assistant",
        "content": "OlÃ¡! ğŸ‘‹ Eu sou o Berserk, um analista de requisitos. Me conte: qual Ã© a principal funcionalidade que vocÃª precisa?"
    }]

if 'llm' not in st.session_state:
    st.session_state.llm = LLMManager()

# CabeÃ§alho centralizado
st.markdown("<h1 style='text-align: center; color: #4B8BBE;'>ğŸ“ Berserk, O Elicitador de Requisitos</h1>", unsafe_allow_html=True)

# Layout em duas colunas
col1, col2 = st.columns([2.5, 1], gap="large")

with col1:
    st.subheader("ğŸ’¬ Conversa com o Assistente")
    chat_container = st.container()
    with chat_container:
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

with col2:
    st.header("Requisitos Identificados")
    requirements = st.session_state.llm.get_requirements()
    
    if not requirements:
        st.info("Nenhum requisito identificado ainda. Continue a conversa para que eu possa extrair os requisitos.")
    else:
        for i, req in enumerate(requirements):
            with st.expander(f"ğŸ§©Requisito #{i+1}"):
                analysis = req['analysis']
                
                # Dividir a anÃ¡lise em seÃ§Ãµes
                sections = analysis.split('\n\n')
                
                # Exibir o requisito
                st.markdown(sections[0])  # Requisito principal
                
                # Exibir histÃ³ria de usuÃ¡rio em um container destacado
                if len(sections) > 1 and 'HistÃ³ria de UsuÃ¡rio:' in sections[1]:
                    with st.container():
                        st.markdown("---")
                        st.markdown("ğŸ“– **HistÃ³ria de UsuÃ¡rio**")
                        historia = sections[1].replace('HistÃ³ria de UsuÃ¡rio:\n', '')
                        st.info(historia)
                
                # Exibir regras de negÃ³cio e critÃ©rios
                for section in sections[2:]:
                    st.markdown(section)
                
                st.caption(f"ğŸ•’ Identificado em: {req['timestamp']}")

# Campo de entrada do usuÃ¡rio
prompt = st.chat_input("Digite sua mensagem...")

if prompt:
    # Armazena a mensagem do usuÃ¡rio
    st.session_state.messages.append({"role": "user", "content": prompt})

    # Exibe mensagem do usuÃ¡rio
    with st.chat_message("user"):
        st.markdown(prompt)

    # Gera resposta do modelo
    with st.chat_message("assistant"):
        response = st.session_state.llm.process_message(prompt)
        st.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response})

# Sidebar com instruÃ§Ãµes
with st.sidebar:
    st.header("â„¹ï¸ Sobre o Elicitador")
    st.markdown("""
    Este assistente ajuda vocÃª a extrair **requisitos de software** de forma orientada.
    
    **Como usar:**
    1. Diga o que o sistema deve fazer
    2. Responda perguntas do assistente
    3. Veja os requisitos extraÃ­dos Ã  direita

    **Exemplos de entrada:**
    - "Quero uma loja virtual que venda roupas"
    - "O usuÃ¡rio precisa fazer login com email e senha"
    """)

    if st.button("ğŸ§¹ Limpar Conversa"):
        st.session_state.messages = [{
            "role": "assistant",
            "content": "OlÃ¡! ğŸ‘‹ Sou um analista de requisitos e vou ajudar vocÃª a definir as funcionalidades do seu sistema. Me conte, qual Ã© a principal funcionalidade que vocÃª precisa?"
        }]
        st.rerun()
